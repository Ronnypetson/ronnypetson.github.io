Sometimes I have the sensation that model-free reinforcement learning is like wanting free lunch. Intelligent behavior in nature doesn't come without some model of the world, I guess.

The vision system in the animals, for instance, is shared through the species by evolution. A new species doesn't start from zero and creates it's own vision system and representations of the world. It inherits the representations of the world and then learn secondary features.

If we want to create AGI as general as human intelligence, we should consider that we have tons of world representations in our minds already.

